\documentclass[runningheads]{llncs}
\newcommand{\lerna}{\textsc{Lerna}}
\newcommand{\z}[1]{\textsc{Z3}}
\begin{document}
\title{A Neural, Parallel Theorem Prover*}
\author{
	Michael Rawson*\orcidID{0000-0001-7834-1567} \and
	Giles Reger
}
\institute{University of Manchester, UK}
\maketitle

\begin{abstract}
	We present the \lerna{} system, a neurally-guided automatic theorem prover for first-order logic with equality.
	The system uses a neural network trained on previous proof search attempts to evaluate subgoals based directly on their structure, and hence bias proof search toward success.
	An existing first-order theorem prover is employed to dispatch easy subgoals and prune branches which cannot be solved.
	Exploration of the search space is asynchronous with respect to both the evaluation network and the existing prover, allowing for efficient batched neural network execution and for natural parallelism within the prover.
\keywords{ATP \and Graph Convolutional Network \and Tableaux \and MCTS}
\end{abstract}

\section{Background}

\subsection{First-Order Logic}
(TODO Giles copy traditional FOL-with-equality background here)

\subsection{Automatic Theorem Provers}
(TODO Giles talk about ATP background/history here)

\subsection{Machine Learning and Theorem Proving}
(TODO talk about the intersection of ATP and ML, in particular with respect to proof search guidance)

TODO The Deep Network Guided Proof Search (DNGPS) system~\cite{DNGPS} should be mentioned.

\section{Introduction}
Recent advances~\cite{graph-cnn,gcn,gcn-relational} in neural network systems allow for processing graph-structured data in a neural context.
Graphs are a natural representation for logical formulae as found in automatic theorem provers~\cite{formula-graph}, suggesting a new breed of \emph{neural ATP} in which proof search is guided by a neural black-box acting as ``mathematician's intuition''.
Much previous work on integrating machine-learned heuristics into automatic theorem provers has relied on hand-engineered features~\cite{MaLeCoP,FEMaLeCoP,rlCoP} or other embedding methods~\cite{ENIGMA}, which have the advantage of simplicity and relative efficiency, but do not encode all information available.
By contrast, a neural method which takes into account all information should allow for greater precision in proof guidance systems.
However, in practice there are several implementation issues which must be avoided in order for neural systems to integrate with efficient traditional ATPs.
\begin{enumerate}
	\item Proof state in such systems may be very large, such as in saturation-based provers~\cite{Vampire}, leading to training data which is impractical to learn from and slow to evaluate. The DNGPS system side-stepped this by only evaluating a single clause and the conjecture, reducing the amount of information available for the neural network in exchange for brevity.
	\item Data structures employed may be very opaque or ``unnatural'', containing artifice designed for efficiency rather than comprehension.
	\item Systems may be very sensitive to latency, which can result in the introduction of neural guidance systems crippling prover throughput and hence performance. Even with the reduced amount of data processed, DNGPS employed a two-phase approach in which the prover ran for some time with network guidance and then for the remainder without guidance.
\end{enumerate}
%
By avoiding these issues with a novel prover architecture and exploring several options to improve efficiency, the \lerna{}\footnote{\textbf{Le}arning to \textbf{R}eason with \textbf{N}eural \textbf{A}rchitectures. Lerna is also the lair of the mythical many-headed beast \textsc{Hydra}.} system explores a step toward useful neural automatic theorem provers.

\section{Design}
\subsection{Rationale}
\subsection{Architecture}
\subsection{Implementation}

\section{Calculus}
\section{Oracle}

\section{Learning}
TODO should ML results go here?

\subsection{Data Collection and Augmentation}
\subsection{Translation to Graphs}
\subsection{Neural Architecture}

\subsection{Neural Network Throughput}
To measure network throughput, the network evaluated \(\approx\) 500MB of pre-recorded data with various batch sizes --- batch size represents a tradeoff between latency of evaluations and parallelism opportunity for a GPU.
Mean throughput (measured in evaluations/second) for each batch size is shown in Table \ref{table:network-throughput}, representing the total number of tableaux sub-goals evaluated in a single second.
As expected, neural network throughput for this task increases rapidly when allowed to process inputs in batches.
In practice a larger batch size was more helpful for proof search, suggesting that a small amount of latency does not significantly affect search in the long term.

\begin{table}
	\caption{Mean neural network throughput versus batch size: evaluations per second.}
	\centering
	\begin{tabular}{c | c}
		\textbf{Batch Size} & \textbf{Mean Throughput (evals./sec)}\\
		\hline
		1 & 254.2\\
		8 & 887.9\\
		32 & 2,128.3\\
		128 & 3,015.0\\
	\end{tabular}
	\label{table:network-throughput}
\end{table}

\section{Results}
\subsection{Hardware}
All results collected on commodity hardware (TODO talk about specs).

\subsection{Benchmark Results}

Table \ref{table:m2k-results} shows the total number of theorems proved using various configurations of \z3 and LeRNA on the M2k dataset.
\z3 ran for a full 10 seconds to establish baseline performance, then as an oracle for 20 milliseconds to determine the number of ``trivial'' problems.
LeRNA ran on an identical dataset, first without guidance from the neural heuristic, then with guidance.

\begin{table}
	\caption{Total theorems proved on the M2k dataset.}
	\centering
	\begin{tabular}{r | c}
		\textbf{Configuration} & \textbf{Theorems Proved}\\
		\hline
		\z3 (10s, as baseline) & 1216\\
		\z3 (20ms, as oracle) & 711\\
		LeRNA, unguided (10s, with oracle) & 969\\
		LeRNA, guided (10s, with oracle) & 1012\\
						      & FIXME there is a bug, these ought be better\\
	\end{tabular}
	\label{table:m2k-results}
\end{table}

\section{Conclusion}
\section{Future Work}
The style of theorem prover described opens many new directions for future work and improvements to the system.
As \lerna{} is a very new system, there is likely much to be gained by simple engineering and tuning: for example, the UCT exploration parameter \(c\) has been left at its theoretical optimum value \(\sqrt{2}\), but it is likely that a higher value will account for neural network inaccuracies and hence improve performance.
Training on, benchmarking with, and optimising for other datasets (such as the famous TPTP benchmark~\cite{TPTP}) is also left as future work.

\subsection{Proof Search}
\lerna{} is well-suited for long-term proof search attempts in mathematics, such as those employed in the AIM project~\cite{AIM}: search is stable over time and does not produce a combinatorial explosion in the same way that some traditional systems tend to after a short period.
Additionally, the amount of information (``confidence'') in the system grows over time, as a result of a growing number of oracle invocations and neural network evaluations.
Proof search can in principle be manually inspected more easily than in saturation-based provers to examine promising subgoals and remove known falsehoods from the search space.
The authors hope to explore applying the system to this interesting domain.

Another future direction for proof search is a principled incomplete mode, in which branches deemed sufficiently uninteresting by the heuristic are pruned, perhaps in response to time or memory constraints as seen in limited resource strategies~\cite{LRS}.
This approach, while clearly incomplete, would significantly accelerate proof search in the direction of more promising search within the available resources.

\subsection{Prover Calculus}
The calculus currently employed is deliberately na\"ive, so much work is planned here.
In particular, the simplification routines can be improved to remove more trivial sub-formulae.
While in general the oracles' pre-processing will remove these, they serve as noise for the neural network and might also increase the number of inference steps required to reach a proof.

As one possible view of this approach is as an intelligent pre-processor for an existing efficient ATP, a greater number of aggressive and/or weakening inferences might be included in the calculus.
It is well-known (TODO find citation) that \emph{prenexing} (or conversely \emph{miniscoping}) formulae can have a significant effect on proof search for some theorem provers, so including suitable quantifier-manipulation rules might prove to be a useful extension.

Generalising further, ideas from other refutation-tableaux calculi could well be suitable for this system.
In particular, the authors are attempting to integrate an adapted connection rule from the non-clausal connection calculus~\cite{non-clausal-connections}, as used in nanoCoP~\cite{nanocop}, in order to reduce the number of proof steps required to instantiate universal quantifiers.

Finally, this prover architecture can support other logics without excessive modification.
Given that \z3 is already capable of supporting many \emph{theories}, such as arithmetic or datatypes, a many-sorted first order logic such as those described by SMTLIB~\cite{smtlib} or the TFF0 dialect of TPTP~\cite{tff0} seems an appropriate first step.

\subsection{Oracle}
While \z3 is a strong theorem prover in its own right and performs well here, it remains to be seen if it is the best for this application.
Other ATPs (or indeed counter-example-finding systems) should be tested as it involves minimal engineering effort.
A \emph{portfolio} of several oracle systems working in tandem might also be considered, although of course this will eventually retard proof search linearly in the number of systems present.

Reducing the number of oracle invocations is another area for optimisation.
Currently, the system calls an oracle for every new subgoal generated.
It seems unlikely that the subgoal is materially easier to dispatch than its parent (especially in the case of propositional inferences that do not split the goal), so heuristically or probabilistically removing such subgoals from the oracle's queue is a possible area for improvement.

\lerna{} does not currently use any information from the oracle beyond its status: using auxiliary information such as satisfying models or unused formulae could well aid proof search.

\subsection{Machine-Learned Heuristic}
Many other graph-based neural architectures are possible.
PyTorch Geometric alone currently includes nearly 40 other graph-specific neural layers pre-programmed from the literature.
Neural models specifically for theorem proving are relatively under-studied.
To combat this, data used for this paper will be published in the near future so that the machine-learning community can improve upon our simple models.

Different approaches to formula-to-graph translation, symbol embeddings, data augmentation, and model integration may also be explored.
\bibliographystyle{splncs04}
\bibliography{references}
\end{document}
