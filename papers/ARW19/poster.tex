\documentclass[24pt, a1paper, portrait]{tikzposter}
\usepackage{enumitem}
\usepackage{graphicx}
\tikzposterlatexaffectionproofoff

\newcommand{\vampire}{\textsc{Vampire}}

\usetheme{Simple}
\definecolorstyle{manchester}
{
	\definecolor{highlight}{rgb}{0.414, 0.171, 0.566}
}
{
	\colorlet{titlebgcolor}{highlight}
	\colorlet{blocktitlefgcolor}{highlight}
}
\usecolorstyle{manchester}

\title{Reinforcement-Learned Input for Saturation}
\author{Michael Rawson, Giles Reger}
\date{September 2019}
\institute{University of Manchester, UK}
 
\begin{document}
\maketitle

\block{Abstract}
{
	Many of today's best-performing automatic theorem provers (ATPs) for first-order logic rely on \emph{saturation} algorithms.
	Saturation algorithms perform all possible inferences within an active set, which yields a complete proof search algorithm.
	However, this produces a large number of clauses, which hinders attempts to guide the prover using neural networks.
	We describe a system which infers new clauses in an incomplete fashion, guided by a neural network.
	The system (in progress) is rewarded if generated clauses are subsequently used by the saturation-based ATP \vampire{} in a proof.
	In this way, the system learns to selectively infer new clauses which are important in proof search, but might not otherwise be found for some time in \vampire{}'s algorithm.

}

\begin{columns}
\column{0.5}
\block{System Description}
{
	The system must use an existing ATP system, in this case \vampire{}.
	On a given first-order problem, it acts as follows:
	\begin{enumerate}[leftmargin=1in]
		\item Clausify an input problem to obtain a clause set.
		\item Loop:
			\begin{enumerate}[leftmargin=1in]
				\item Neurally select an inference between two clauses.
				\item Perform the inference (using the ATP)
				\item Add any generated clauses to the clause set.
			\end{enumerate}
		\item Repeat until a time or inference threshold is met.
		\item Invoke the ATP on the new clause set and run as usual.
	\end{enumerate}
	\vspace{.5in}
	This retains the advantages of advanced clausification and theorem-proving technology (such as completeness and advanced redundancy eliminations), but with the addition of a neural pre-processing step.
}

\block{Reinforcement and Reward}
{
	It is unclear at present what reward function should be chosen for the network, but the reward can be significantly more expressive than any manual heuristic can be.

	Examples of inferences to reward include:
	\begin{itemize}[leftmargin=1in]
		\item Clauses included in the eventual proof (current).
		\item Only clauses used more than once in the proof.
		\item Clauses which subsume other clauses.
		\item Ground clauses (which can be used for splitting).
		\item Any weighted combination of the above.
	\end{itemize}
	\vspace{.5in}
	Since the clause space can be very large even on small problems, supervised learning is not very practical here.
	Instead, reinforcement learning techniques may be leveraged, providing training data by self-play.
	The current prototype simply generates clause sets by self-play and then trains the network to identify clauses used in the proof.
}

\column{0.5}
\block{Graphs for Clause Processing}
{
	{
	\centering
	\includegraphics[width=10in]{clause-graph}
	}
	The complex directed graph shown is a compact, lossless representation of TPTP problem \texttt{PUZ001+1}, or ``who killed Aunt Agatha?'', with perfect term sharing.
	Graph neural networks can process such graphs by sharing information between adjacent nodes.
	This is a convenient end-to-end method for processing clauses in a neural context.
}

\block{Future Work}
{
	Much is still left to do:
	\begin{itemize}[leftmargin=1in]
		\item Finish training and evaluate a prototype.
		\item Examine the effect on proof search in \vampire{}.
		\item Determine good strategies for the modified clause sets generated.
		\item Experiment with other reward functions and reinforcement learning techniques, such as \(Q\)-learning.
	\end{itemize}
}
\end{columns}

\end{document}
